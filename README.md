# 深度学习课程学期总结报告

## 第一章     课程学习总结

### 1.1 所学主要知识点概述

| 模型名称 | 模型应用场景 | 模型优点 | 模型缺点 |
| --------- | -------------- | --------- |---------- |
| Linear Regression  | 房价预测、股票趋势分析、销售额预估等简单数值预测场景 | 原理简单易懂，计算效率高 | 仅能处理线性关系，无法捕捉复杂非线性模式 |
| Logistic Regression | 垃圾邮件分类、疾病检测、用户是否点击广告等二分类任务 | 模型简单高效，能输出分类概率 | 仅适用于二分类问题，对非线性关系处理能力有限 |
| MLP | 图像简单分类（如 MNIST 手写数字识别）、文本基础分类、简单模式识别 | 可处理非线性问题，理论上能拟合任意复杂函数，结构灵活易实现 | 全连接层参数量大，容易过拟合，对数据预处理要求较高，训练耗时 |
| CNN | 图像分类（如 ImageNet）、目标检测（如 YOLO）、语义分割、视频分析 | 能自动提取空间特征，卷积操作可减少参数数量，对平移、旋转等变换具有不变性 | 对序列数据（如长文本）处理能力较弱，难以捕捉全局依赖关系，计算复杂度较高 |
| RNN | 文本生成（如诗词创作）、语音识别、时间序列预测（如股票走势） | 适合处理序列数据，能捕捉序列中的时序依赖关系，结构设计符合序列处理逻辑 | 存在长期依赖问题，梯度容易消失或爆炸，无法并行处理长序列，训练效率低 |
| LSTM | 机器翻译、情感分析、视频字幕生成、复杂时间序列预测 | 通过门控机制解决 RNN 长期依赖问题，能有效捕捉长距离时序依赖 | 结构复杂，参数数量多，训练耗时较长，计算成本高于简单 RNN |
| GRU | 文本摘要生成、语音情感识别、短文本序列处理| 作为 LSTM 简化版，参数更少，训练速度更快，计算效率高，在多数序列任务中性能接近 LSTM | 捕捉长期依赖的能力略弱于 LSTM，门控机制相对简单 |
| Transformer | 机器翻译（如 Google Translate）、文本生成（如 GPT 系列）、问答系统、多模态任务 | 完全基于注意力机制，可并行计算，能高效捕捉全局依赖关系，推动 NLP 进入预训练时代 | 对长序列计算复杂度高（平方级），缺乏时序归纳偏置，初期训练需要大量数据 |
| GANS | 图像生成（如人脸、风景）、风格迁移（如照片转油画）、数据增强、虚拟人物创建 | 生成样本质量高，可捕捉数据分布的潜在规律，无需显式定义损失函数 | 训练过程不稳定（模式崩溃问题），生成结果不可控，评估指标不完善 |
| VAE | 图像压缩、无监督特征学习、生成式建模（如人脸插值）、数据降维 | 基于概率模型，生成样本具有连续性，可解释性较强，适合无监督学习场景 | 生成样本清晰度可能低于 GAN，优化目标包含 KL 散度约束，存在模糊化倾向 |
